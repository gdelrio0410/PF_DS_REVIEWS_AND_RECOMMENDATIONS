
# üçΩÔ∏è Proyecto de Inversi√≥n Gastron√≥mica y Afines en el Mercado de Estados Unidos üá∫üá∏

## ROLES Y RESPONSABILIDADES: üë®‚Äçüíºüë®‚Äçüíºüë®‚Äçüíºüë®‚Äçüíºüë®‚Äçüíº

- V√≠ctor Vargas --> Data Engineer
- Guillermo del R√≠o --> Data Science
- Michael Mart√≠nez --> Data Engineer
- Juli√°n Scarpeccio --> Data Science
- Benjam√≠n Zelaya --> Data Analyst

## Entendimiento de la situaci√≥n propuesta ü§î

### Descripci√≥n del Proyecto

El cliente, que forma parte de un conglomerado de empresas en el sector de restaurantes y afines, busca realizar un an√°lisis exhaustivo de las opiniones de los usuarios en las plataformas Yelp y Google Maps. Este an√°lisis se centrar√° en hoteles, restaurantes y otros negocios relacionados con el turismo y el ocio. El cliente tambi√©n est√° interesado en identificar ubicaciones estrat√©gicas para
abrir nuevos locales de restaurantes y negocios relacionados, as√≠ como desarrollar un sistema de recomendaci√≥n de restaurantes que permita a los usuarios descubrir nuevas experiencias basadas en sus preferencias previas.

Para llevar a cabo este proyecto, el cliente ha contratado a Icon Data Science Consulting como consultores externos. La tarea principal es estudiar la relaci√≥n entre las rese√±as de diferentes tipos de negocios, que incluyen restaurantes, hoteles, y otros servicios. Adem√°s, se analizar√°n datos como la cantidad de locales de restaurantes, sus nombres y franquicias, as√≠ como el tipo de oferta gastron√≥mica que ofrecen. El objetivo es evaluar la viabilidad de introducir una propuesta gastron√≥mica completamente nueva o incorporar una franquicia existente en ubicaciones estrat√©gicas.

## Objetivos del Proyecto

### Objetivos del Trabajo:
- Recopilar y depurar datos de diferentes fuentes para crear una base de datos.
- Realizar un an√°lisis exploratorio de los datos para encontrar relaciones.
- Crear un dashboard interactivo y visualmente atractivo que integre los resultados del an√°lisis exploratorio de datos.
- Entrenar y poner en producci√≥n un modelo de machine learning para proponer una oportunidad de inversi√≥n en el sector.

### Objetivos del Grupo:
- Investigar y analizar los conjuntos de datos de locales gastron√≥micos y sus franquicias en cada uno de los conglomerados para identificar tendencias.
- Entender la correlaci√≥n entre los tipos de locales gastron√≥micos, franquicias y reviews con el fin de proporcionar una base s√≥lida para las decisiones futuras relacionadas con la oportunidad de inversi√≥n.
- Proveer informaci√≥n relevante y confiable a la empresa para respaldar su toma de decisi√≥n sobre el emplazamiento de un nuevo local
comercial.


## An√°lisis preliminar de Calidad del dato üìä

Se realiz√≥ un an√°lisis preliminar y transformaciones de los conjuntos de datos dados por la empresa y obtenidos externamente para as√≠ observar la calidad de los datos, concatenar archivos, limpiar valores nulos, observar tipos de datos y realizar gr√°ficos que nos ayuden a comprender la informaci√≥n. Podemos observar los archivos dentro de la carpeta sprint_1 llamado ETL y EDA.

## KPI

- Tasa de Satisfacci√≥n del Cliente

Este KPI mide el porcentaje de clientes satisfechos en funci√≥n de las rese√±as y opiniones recopiladas en plataformas como Yelp y Google Maps.
Tasa de Satisfacci√≥n del Cliente = (N√∫mero de Rese√±as Positivas / Total de Rese√±as) x 100

- Rentabilidad por Ubicaci√≥n

Permite identificar cu√°les son las ubicaciones m√°s rentables y cu√°les pueden requerir mejoras.
Rentabilidad por Ubicaci√≥n = (Ingresos - Costos) / Ingresos x 100

- Porcentaje de Restaurantes con Alta Calificaci√≥n

Este KPI muestra el porcentaje de restaurantes que tienen una calificaci√≥n por encima de un umbral espec√≠fico (por ejemplo, 4 estrellas).
Porcentaje de Restaurantes con Alta Calificaci√≥n = (N√∫mero de Restaurantes con Rating >= Umbral) / Total de Restaurantes x 100

- Diferencia Promedio de Rating por Estado

identificar si la calidad de los restaurantes var√≠a significativamente de un estado a otro.
Diferencia Promedio de Rating por Estado = Promedio de "Rating" agrupado por "Estado".

## Metodologia propuesta

<br>
<div style="text-align: center;">
  <img src='./Images/metodolog√≠a-scrum.png' alt="imagen data pipeline">
</div>
<br>

En este proyecto, estamos empleando la metodolog√≠a √°gil mediante el marco de trabajo Scrum utilizando el software de administraci√≥n de proyectos trello. Esto conlleva la incorporaci√≥n de los valores y conceptos √°giles en nuestro enfoque de desarrollo. Estamos aplicando la estructura definida por Scrum en t√©rminos de roles, eventos, artefactos y reglas para la organizaci√≥n y gesti√≥n efectiva del trabajo colaborativo.


## **Stack tecnol√≥gico**

Para llevar a cabo nuestro proyecto hemos seleccionado las siguientes tecnolog√≠as:

- Trabajo diario: python, google meet, github.

- Ingenier√≠a de datos: Python, mysql, Docker, Apache Airflow.

- An√°lisis y visualizaci√≥n de datos: Power Bi, python.

- Modelo de machine learning: Python.

- Gesti√≥n de proyectos: Jira


## Data Pipeline

En esta secci√≥n se estructurar√° el flujo de datos desde la recepci√≥n hasta la salida del ETL.

<br>
<div style="text-align: center;">
  <img src='./Images/pipeline_diagram.png' alt="imagen data pipeline">
</div>
<br>

### Data Ingest
Los datos entregados por la empresa y extra√≠dos por nuestro equipo mediante api y web scraping se descargaron y son almacenados de manera temporal en el localhost de nuestra m√°quina.
Dado que trabajaremos sobre el esquema de Microsoft Azure se crear√° un contenedor donde se almacenar√°n los datasets sin procesar en la nube. Para esto, fue necesario crear una cuenta de trabajo en el portal de Azure. En dicha cuenta se crea un grupo de recursos donde inclu√≠mos una cuenta de almacenamiento con un contenedor.

### Conexi√≥n con Databricks
Una vez almacenados los datasets en el contenedor de Azure se procede a realizar la conexi√≥n con Databricks, nuestro lugar de trabajo principal.
En el grupo de recursos previamente creado se a√±ade un workspace de Databricks, Ah√≠ se crear√° un cl√∫ster que permite computar nuestros datos (Single Node 10.4 LTS Apache Spark 14 GB Memory, 4 Cores), el criterio de selecci√≥n es en base al alcance de nuestros recursos.
Dentro de Databricks creamos un Notebook y lo conectamos con el cl√∫ster. En dicho Notebook establecemos las variables necesarias para la conexi√≥n con el contenedor.

### ETL
Se realizar√° todo el proceso de extracci√≥n, transformaci√≥n y carga de los datos hacia el data warehouse.

### Conexi√≥n con sql database
Creada la SQL Database de Azure se realizar√° la conexi√≥n con Databricks por medio del protocolo jdbc.

### Conexi√≥n con power bi
La conexi√≥n se realiza mediante el conector de Azure SQL Database de PowerBI. Se ingresan las credenciales del servidor de base de datos y se cargan los datos ya sea por Direct Query o Import Data.

